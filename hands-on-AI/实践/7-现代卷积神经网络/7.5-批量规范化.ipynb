{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练深层网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量规范化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not torch.is_grad_enabled():\n",
    "        X_hat = (X-moving_mean) / torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        match len(X.shape):\n",
    "            case 2:\n",
    "                mean = X.mean(dim=0)\n",
    "                var = ((X-mean)**2).mean(dim=0)\n",
    "            case 4:\n",
    "                mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "                var = ((X-mean)**2).mean(dim=(0,2,3), keepdim=True)\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid X shape dims: {len(X.shape)}\")\n",
    "        X_hat = (X-mean) / torch.sqrt(var+eps)\n",
    "        moving_mean = momentum*moving_mean + (1.0-momentum)*mean\n",
    "        moving_var = momentum*moving_var + (1.0-momentum)*var\n",
    "    Y = gamma*X_hat + beta\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\n(tensor([[[[ 0.,  1.,  2.],\\n           [ 3.,  4.,  5.]],\\n \\n          [[ 6.,  7.,  8.],\\n           [ 9., 10., 11.]]],\\n \\n \\n         [[[12., 13., 14.],\\n           [15., 16., 17.]],\\n \\n          [[18., 19., 20.],\\n           [21., 22., 23.]]]]),\\n tensor([[[[ 8.5000]],\\n \\n          [[14.5000]]]]),\\n torch.Size([1, 2, 1, 1]))\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"example\n",
    "X = torch.arange(24, dtype=torch.float32).reshape(2, 2, 2, 3)\n",
    "X, X.mean(dim=(0, 2, 3), keepdim=True), X.mean(dim=(0, 2, 3), keepdim=True).shape\n",
    "\"\"\"\n",
    "\"\"\"output\n",
    "(tensor([[[[ 0.,  1.,  2.],\n",
    "           [ 3.,  4.,  5.]],\n",
    " \n",
    "          [[ 6.,  7.,  8.],\n",
    "           [ 9., 10., 11.]]],\n",
    " \n",
    " \n",
    "         [[[12., 13., 14.],\n",
    "           [15., 16., 17.]],\n",
    " \n",
    "          [[18., 19., 20.],\n",
    "           [21., 22., 23.]]]]),\n",
    " tensor([[[[ 8.5000]],\n",
    " \n",
    "          [[14.5000]]]]),\n",
    " torch.Size([1, 2, 1, 1]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        shape = (1, num_features) if num_dims == 2 else (1, num_features, 1, 1)\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma, self.beta, self.moving_mean,\n",
    "                                                          self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用批量规范化的LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\nConv2d output shape: \\ttorch.Size([1, 6, 24, 24])\\nBatchNorm output shape: \\ttorch.Size([1, 6, 24, 24])\\nSigmoid output shape: \\ttorch.Size([1, 6, 24, 24])\\nAvgPool2d output shape: \\ttorch.Size([1, 6, 12, 12])\\nConv2d output shape: \\ttorch.Size([1, 16, 8, 8])\\nBatchNorm output shape: \\ttorch.Size([1, 16, 8, 8])\\nSigmoid output shape: \\ttorch.Size([1, 16, 8, 8])\\nAvgPool2d output shape: \\ttorch.Size([1, 16, 4, 4])\\nFlatten output shape: \\ttorch.Size([1, 256])\\nLinear output shape: \\ttorch.Size([1, 120])\\nBatchNorm output shape: \\ttorch.Size([1, 120])\\nSigmoid output shape: \\ttorch.Size([1, 120])\\nLinear output shape: \\ttorch.Size([1, 84])\\nBatchNorm output shape: \\ttorch.Size([1, 84])\\nSigmoid output shape: \\ttorch.Size([1, 84])\\nLinear output shape: \\ttorch.Size([1, 10])\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"example\n",
    "X = torch.randn(size=(1,1,28,28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, f'output shape: \\t{X.shape}')\n",
    "\"\"\"\n",
    "\"\"\"output\n",
    "Conv2d output shape: \ttorch.Size([1, 6, 24, 24])\n",
    "BatchNorm output shape: \ttorch.Size([1, 6, 24, 24])\n",
    "Sigmoid output shape: \ttorch.Size([1, 6, 24, 24])\n",
    "AvgPool2d output shape: \ttorch.Size([1, 6, 12, 12])\n",
    "Conv2d output shape: \ttorch.Size([1, 16, 8, 8])\n",
    "BatchNorm output shape: \ttorch.Size([1, 16, 8, 8])\n",
    "Sigmoid output shape: \ttorch.Size([1, 16, 8, 8])\n",
    "AvgPool2d output shape: \ttorch.Size([1, 16, 4, 4])\n",
    "Flatten output shape: \ttorch.Size([1, 256])\n",
    "Linear output shape: \ttorch.Size([1, 120])\n",
    "BatchNorm output shape: \ttorch.Size([1, 120])\n",
    "Sigmoid output shape: \ttorch.Size([1, 120])\n",
    "Linear output shape: \ttorch.Size([1, 84])\n",
    "BatchNorm output shape: \ttorch.Size([1, 84])\n",
    "Sigmoid output shape: \ttorch.Size([1, 84])\n",
    "Linear output shape: \ttorch.Size([1, 10])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"下载Fashion-MNIST数据集, 然后将其加载到内存中\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=True, transform=trans, download=True\n",
    "    )\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../data\", train=False, transform=trans, download=True\n",
    "    )\n",
    "    return (\n",
    "        data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "        data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [.0] * n\n",
    "    def add(self, *args):\n",
    "        self.data = [a+float(b) for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data = [.0] * len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(net.parameters()).device\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"记录多次运行时间\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "    def start(self):\n",
    "        \"\"\"启动计时器\"\"\"\n",
    "        self.tik = time.time()\n",
    "    def stop(self):\n",
    "        \"\"\"停止计时器并将时间记录在列表中\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "    def avg(self):\n",
    "        \"\"\"返回平均时间\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "    def sum(self):\n",
    "        \"\"\"返回时间总和\"\"\"\n",
    "        return sum(self.times)\n",
    "    def cumsum(self):\n",
    "        \"\"\"返回累计时间\"\"\"\n",
    "        return np.cumsum(self.times).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:\n",
    "    \"\"\"在动画中绘制数据\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, xlim=None, ylim=None,\n",
    "                xscale='linear', yscale='linear', legend=None,\n",
    "                fmts=('-', 'm--', 'g-.', 'r:')):\n",
    "        ## 增量地绘制多条线\n",
    "        legend = [] if legend is None else legend\n",
    "        self.fig, self.axes = plt.gcf(), plt.gca()\n",
    "        self.config_axes = lambda: self.set_axes(xlabel, ylabel, xlim, ylim, xscale, yscale, legend) ## 使用lambda表达式捕获变量\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def set_axes(self, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        self.axes.set_xlabel(xlabel)\n",
    "        self.axes.set_ylabel(ylabel)\n",
    "        self.axes.set_xlim(xlim)\n",
    "        self.axes.set_ylim(ylim)\n",
    "        self.axes.set_xscale(xscale)\n",
    "        self.axes.set_yscale(yscale)\n",
    "        self.axes.legend(legend)\n",
    "        self.axes.grid()\n",
    "\n",
    "    def add(self, x, y):\n",
    "        \"\"\"向图表中添加多个数据点\"\"\"\n",
    "        y = [y] if not hasattr(y, \"__len__\") else y\n",
    "        x = [x] * len(y) if not hasattr(x, \"__len__\") else x\n",
    "        self.X = [[] for _ in range(len(y))] if self.X is None else self.X\n",
    "        self.Y = [[] for _ in range(len(y))] if self.Y is None else self.Y\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes.cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes.plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    \"\"\"使用GPU训练模型\"\"\"\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = Animator(xlabel='epoch', xlim=[1,num_epochs], \n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X,y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l*X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i+1) % (num_batches//5) == 0 or i == num_batches-1:\n",
    "                animator.add(epoch+(i+1)/num_batches, (train_l, train_acc, None))\n",
    "            test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "            animator.add(epoch+1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2]*num_epochs/timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 1.0, 10\n",
    "train(net, train_iter, test_iter, num_epochs, lr, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), \n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16*4*4, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 1.0, 10\n",
    "train(net, train_iter, test_iter, num_epochs, lr, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 争议"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在使用批量规范化之前，我们是否可以从全连接层或卷积层中删除偏置参数？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 比较LeNet在使用和不使用批量规范化情况下的学习率\n",
    "    - 绘制训练和测试准确度的提高\n",
    "    - 学习率有多高？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 我们是否需要在每个层中进行批量规范化？尝试一下？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 可以通过批量规范化来替换暂退法吗？行为会如何改变？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 确定参数beta和gamma，并观察和分析结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 查看高级API中有关BatchNorm的在线文档，以查看其他批量规范化的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 研究思路：可以应用的其他“规范化”转换？可以应用概率积分变换吗？全秩协方差估计可以么？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
