{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        ## 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "    def forward(self, X, *args):\n",
    "        ## 输出X的形状 (batch_size, num_steps, embed_size)\n",
    "        X = self.embedding(X)\n",
    "        ## 在循环神经网络模型中,第一个轴对应时间步\n",
    "        X = X.permute(1, 0, 2)\n",
    "        output, state = self.rnn(X)\n",
    "        return (output, state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\n(tensor([[[ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11]],\\n \\n         [[12, 13, 14, 15],\\n          [16, 17, 18, 19],\\n          [20, 21, 22, 23]]]),\\n tensor([[[ 0,  1,  2,  3],\\n          [12, 13, 14, 15]],\\n \\n         [[ 4,  5,  6,  7],\\n          [16, 17, 18, 19]],\\n \\n         [[ 8,  9, 10, 11],\\n          [20, 21, 22, 23]]]),\\n tensor([[[ 0,  1,  2,  3],\\n          [12, 13, 14, 15]],\\n \\n         [[ 4,  5,  6,  7],\\n          [16, 17, 18, 19]],\\n \\n         [[ 8,  9, 10, 11],\\n          [20, 21, 22, 23]]]))\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"example\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "permute_X = X.permute(1, 0, 2)\n",
    "transpose_X = X.transpose(1, 0)\n",
    "X, permute_X, transpose_X\n",
    "\"\"\"\n",
    "\"\"\"output\n",
    "(tensor([[[ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11]],\n",
    " \n",
    "         [[12, 13, 14, 15],\n",
    "          [16, 17, 18, 19],\n",
    "          [20, 21, 22, 23]]]),\n",
    " tensor([[[ 0,  1,  2,  3],\n",
    "          [12, 13, 14, 15]],\n",
    " \n",
    "         [[ 4,  5,  6,  7],\n",
    "          [16, 17, 18, 19]],\n",
    " \n",
    "         [[ 8,  9, 10, 11],\n",
    "          [20, 21, 22, 23]]]),\n",
    " tensor([[[ 0,  1,  2,  3],\n",
    "          [12, 13, 14, 15]],\n",
    " \n",
    "         [[ 4,  5,  6,  7],\n",
    "          [16, 17, 18, 19]],\n",
    " \n",
    "         [[ 8,  9, 10, 11],\n",
    "          [20, 21, 22, 23]]]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(\n",
    "    vocab_size=10,\n",
    "    embed_size=8,\n",
    "    num_hiddens=16,\n",
    "    num_layers=2\n",
    ")\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape, state.shape   ## state的shape为(隐藏层数量，批量大小，隐藏单元数量)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size+num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\n(tensor([[[ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11]]]),\\n tensor([[[ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11]],\\n \\n         [[ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11]]]),\\n tensor([[[ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11],\\n          [ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11]],\\n \\n         [[ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11],\\n          [ 0,  1,  2,  3],\\n          [ 4,  5,  6,  7],\\n          [ 8,  9, 10, 11]]]))\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"example\n",
    "a = torch.arange(12).reshape(1, 3, 4)\n",
    "a, a.repeat(2, 1, 1), a.repeat(2, 2, 1) \n",
    "\"\"\"\n",
    "\"\"\"output\n",
    "(tensor([[[ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11]]]),\n",
    " tensor([[[ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11]],\n",
    " \n",
    "         [[ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11]]]),\n",
    " tensor([[[ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11],\n",
    "          [ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11]],\n",
    " \n",
    "         [[ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11],\n",
    "          [ 0,  1,  2,  3],\n",
    "          [ 4,  5,  6,  7],\n",
    "          [ 8,  9, 10, 11]]]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中遮蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None,:] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\ntensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"example\n",
    "a = torch.arange(12)[None,:]\n",
    "a\n",
    "\"\"\"\n",
    "\"\"\"output\n",
    "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output\\ntensor([[-1, -1, -1, -1, -1, -1],\\n        [ 6,  7,  8,  9, 10, 11]])\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"example\n",
    "a = torch.arange(12).reshape(2,6)\n",
    "a[0] = -1\n",
    "a\n",
    "\"\"\"\n",
    "\"\"\"output\n",
    "tensor([[-1, -1, -1, -1, -1, -1],\n",
    "        [ 6,  7,  8,  9, 10, 11]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3],[4,5,6]])\n",
    "sequence_mask(X, torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2,3,4)\n",
    "sequence_mask(X, torch.tensor([1,2]), value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    ## pred      --> (batch_size, num_steps, vocab_size)\n",
    "    ## label     --> (batch_size, num_steps)\n",
    "    ## valid_len --> (batch_size, )\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label\n",
    "        )\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3,4), dtype=torch.long), torch.tensor([4,2,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
