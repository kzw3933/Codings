{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标量\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x+y, x*y, x/y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor(3), 4, 1, torch.Size([4]), torch.Size([4]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x, x[3], len(x), x.ndim, x.shape, x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15],\n",
       "         [16, 17, 18, 19]]),\n",
       " tensor([[ 0,  4,  8, 12, 16],\n",
       "         [ 1,  5,  9, 13, 17],\n",
       "         [ 2,  6, 10, 14, 18],\n",
       "         [ 3,  7, 11, 15, 19]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(20).reshape(5,4)\n",
    "x, x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24).reshape(2,3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8628,  1.2055,  1.0282],\n",
       "         [ 0.6488,  1.9246,  1.0194]]),\n",
       " tensor([[-1.8628,  0.2055,  0.0282],\n",
       "         [-0.3512,  0.9246,  0.0194]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "y = x.clone()\n",
    "x += torch.ones_like(x)\n",
    "x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'T', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__cuda_array_interface__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__dlpack__', '__dlpack_device__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__', '__ilshift__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__torch_dispatch__', '__torch_function__', '__truediv__', '__weakref__', '__xor__', '_addmm_activation', '_autocast_to_full_precision', '_autocast_to_reduced_precision', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_conj', '_conj_physical', '_dimI', '_dimV', '_fix_weakref', '_grad', '_grad_fn', '_has_symbolic_sizes_strides', '_indices', '_is_all_true', '_is_any_true', '_is_view', '_is_zerotensor', '_make_subclass', '_make_wrapper_subclass', '_neg_view', '_nested_tensor_size', '_nested_tensor_storage_offsets', '_nested_tensor_strides', '_nnz', '_post_accumulate_grad_hooks', '_python_dispatch', '_reduce_ex_internal', '_sparse_mask_projection', '_to_dense', '_to_sparse', '_to_sparse_bsc', '_to_sparse_bsr', '_to_sparse_csc', '_to_sparse_csr', '_typed_storage', '_update_names', '_values', '_version', '_view_func', 'abs', 'abs_', 'absolute', 'absolute_', 'acos', 'acos_', 'acosh', 'acosh_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'adjoint', 'align_as', 'align_to', 'all', 'allclose', 'amax', 'amin', 'aminmax', 'angle', 'any', 'apply_', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan2', 'arctan2_', 'arctan_', 'arctanh', 'arctanh_', 'argmax', 'argmin', 'argsort', 'argwhere', 'as_strided', 'as_strided_', 'as_strided_scatter', 'as_subclass', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan2_', 'atan_', 'atanh', 'atanh_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bfloat16', 'bincount', 'bitwise_and', 'bitwise_and_', 'bitwise_left_shift', 'bitwise_left_shift_', 'bitwise_not', 'bitwise_not_', 'bitwise_or', 'bitwise_or_', 'bitwise_right_shift', 'bitwise_right_shift_', 'bitwise_xor', 'bitwise_xor_', 'bmm', 'bool', 'broadcast_to', 'byte', 'cauchy_', 'ccol_indices', 'cdouble', 'ceil', 'ceil_', 'cfloat', 'chalf', 'char', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clip', 'clip_', 'clone', 'coalesce', 'col_indices', 'conj', 'conj_physical', 'conj_physical_', 'contiguous', 'copy_', 'copysign', 'copysign_', 'corrcoef', 'cos', 'cos_', 'cosh', 'cosh_', 'count_nonzero', 'cov', 'cpu', 'cross', 'crow_indices', 'cuda', 'cummax', 'cummin', 'cumprod', 'cumprod_', 'cumsum', 'cumsum_', 'data', 'data_ptr', 'deg2rad', 'deg2rad_', 'dense_dim', 'dequantize', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'diagonal_scatter', 'diff', 'digamma', 'digamma_', 'dim', 'dim_order', 'dist', 'div', 'div_', 'divide', 'divide_', 'dot', 'double', 'dsplit', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp2', 'exp2_', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fill_', 'fill_diagonal_', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'float_power', 'float_power_', 'floor', 'floor_', 'floor_divide', 'floor_divide_', 'fmax', 'fmin', 'fmod', 'fmod_', 'frac', 'frac_', 'frexp', 'gather', 'gcd', 'gcd_', 'ge', 'ge_', 'geometric_', 'geqrf', 'ger', 'get_device', 'grad', 'grad_fn', 'greater', 'greater_', 'greater_equal', 'greater_equal_', 'gt', 'gt_', 'half', 'hardshrink', 'has_names', 'heaviside', 'heaviside_', 'histc', 'histogram', 'hsplit', 'hypot', 'hypot_', 'i0', 'i0_', 'igamma', 'igamma_', 'igammac', 'igammac_', 'imag', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_reduce', 'index_reduce_', 'index_select', 'indices', 'inner', 'int', 'int_repr', 'inverse', 'ipu', 'is_coalesced', 'is_complex', 'is_conj', 'is_contiguous', 'is_cpu', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_inference', 'is_ipu', 'is_leaf', 'is_meta', 'is_mkldnn', 'is_mps', 'is_neg', 'is_nested', 'is_nonzero', 'is_ort', 'is_pinned', 'is_quantized', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'is_sparse_csr', 'is_vulkan', 'is_xla', 'is_xpu', 'isclose', 'isfinite', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'item', 'itemsize', 'kron', 'kthvalue', 'layout', 'lcm', 'lcm_', 'ldexp', 'ldexp_', 'le', 'le_', 'lerp', 'lerp_', 'less', 'less_', 'less_equal', 'less_equal_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_and_', 'logical_not', 'logical_not_', 'logical_or', 'logical_or_', 'logical_xor', 'logical_xor_', 'logit', 'logit_', 'logsumexp', 'long', 'lstsq', 'lt', 'lt_', 'lu', 'lu_solve', 'mH', 'mT', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_exp', 'matrix_power', 'max', 'maximum', 'mean', 'median', 'min', 'minimum', 'mm', 'mode', 'moveaxis', 'movedim', 'msort', 'mul', 'mul_', 'multinomial', 'multiply', 'multiply_', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'names', 'nan_to_num', 'nan_to_num_', 'nanmean', 'nanmedian', 'nanquantile', 'nansum', 'narrow', 'narrow_copy', 'nbytes', 'ndim', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'negative', 'negative_', 'nelement', 'new', 'new_empty', 'new_empty_strided', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nextafter', 'nextafter_', 'nonzero', 'nonzero_static', 'norm', 'normal_', 'not_equal', 'not_equal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'outer', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'positive', 'pow', 'pow_', 'prelu', 'prod', 'put', 'put_', 'q_per_channel_axis', 'q_per_channel_scales', 'q_per_channel_zero_points', 'q_scale', 'q_zero_point', 'qr', 'qscheme', 'quantile', 'rad2deg', 'rad2deg_', 'random_', 'ravel', 'real', 'reciprocal', 'reciprocal_', 'record_stream', 'refine_names', 'register_hook', 'register_post_accumulate_grad_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'rename', 'rename_', 'renorm', 'renorm_', 'repeat', 'repeat_interleave', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'resize_as_sparse_', 'resolve_conj', 'resolve_neg', 'retain_grad', 'retains_grad', 'roll', 'rot90', 'round', 'round_', 'row_indices', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'scatter_reduce', 'scatter_reduce_', 'select', 'select_scatter', 'set_', 'sgn', 'sgn_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'signbit', 'sin', 'sin_', 'sinc', 'sinc_', 'sinh', 'sinh_', 'size', 'slice_scatter', 'slogdet', 'smm', 'softmax', 'solve', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'square', 'square_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'subtract', 'subtract_', 'sum', 'sum_to_size', 'svd', 'swapaxes', 'swapaxes_', 'swapdims', 'swapdims_', 'symeig', 't', 't_', 'take', 'take_along_dim', 'tan', 'tan_', 'tanh', 'tanh_', 'tensor_split', 'tile', 'to', 'to_dense', 'to_mkldnn', 'to_padded_tensor', 'to_sparse', 'to_sparse_bsc', 'to_sparse_bsr', 'to_sparse_coo', 'to_sparse_csc', 'to_sparse_csr', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'triangular_solve', 'tril', 'tril_', 'triu', 'triu_', 'true_divide', 'true_divide_', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unflatten', 'unfold', 'uniform_', 'unique', 'unique_consecutive', 'unsafe_chunk', 'unsafe_split', 'unsafe_split_with_sizes', 'unsqueeze', 'unsqueeze_', 'untyped_storage', 'values', 'var', 'vdot', 'view', 'view_as', 'volatile', 'vsplit', 'where', 'xlogy', 'xlogy_', 'xpu', 'zero_']\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch.Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " tensor([60, 66, 72, 78]),\n",
       " tensor([[12, 14, 16, 18],\n",
       "         [20, 22, 24, 26],\n",
       "         [28, 30, 32, 34]]),\n",
       " tensor([[[12, 14, 16, 18],\n",
       "          [20, 22, 24, 26],\n",
       "          [28, 30, 32, 34]]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24).reshape(2,3,4)\n",
    "x, x.sum(axis=[0,1]), x.sum(axis=0), x.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.]],\n",
       " \n",
       "         [[12., 13., 14., 15.],\n",
       "          [16., 17., 18., 19.],\n",
       "          [20., 21., 22., 23.]]]),\n",
       " tensor([[ 6.,  7.,  8.,  9.],\n",
       "         [10., 11., 12., 13.],\n",
       "         [14., 15., 16., 17.]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24, dtype=torch.float32).reshape(2,3,4)\n",
    "x, x.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 14, 16, 18],\n",
       "          [20, 22, 24, 26],\n",
       "          [28, 30, 32, 34]]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(24).reshape(2,3,4)\n",
    "y = x.cumsum(axis=0)   ## 沿某个轴累积求和  \n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1390, -0.3164, -2.1334, -1.2904],\n",
       "         [-0.4456,  1.5825,  0.0880, -1.0508],\n",
       "         [ 2.0032,  0.7315,  0.8971,  1.1423],\n",
       "         [-0.2390,  0.7601, -0.0116,  0.7875],\n",
       "         [ 1.5335, -0.0338,  0.8940,  1.4516]]),\n",
       " tensor([-0.8499,  0.3322,  2.4755,  1.8914]),\n",
       " tensor([-7.7088, -0.8653,  2.9218,  1.9163,  3.6441]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(20).reshape(5,4)\n",
    "x = torch.randn(4)\n",
    "A, x, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4288,  0.6844, -0.0117,  0.6614],\n",
       "         [-0.1678, -1.3514, -0.1151, -0.1543],\n",
       "         [ 0.2425, -1.3238, -0.0081, -0.3247],\n",
       "         [ 0.3004, -0.0336, -0.5094, -0.3208],\n",
       "         [-0.3908,  1.5206,  1.8827, -1.9798]]),\n",
       " tensor([[ 0.1974, -0.1115, -0.6971],\n",
       "         [ 0.6615, -0.3760, -0.7031],\n",
       "         [ 0.2590, -0.2566, -0.4986],\n",
       "         [ 0.3312, -1.0925,  0.9487]]),\n",
       " tensor([[ 0.7533, -1.0247, -0.1468],\n",
       "         [-1.0080,  0.7250,  0.9782],\n",
       "         [-0.9375,  0.8275,  0.4577],\n",
       "         [-0.2011,  0.4603, -0.2361],\n",
       "         [ 0.7608,  1.1515, -3.6137]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(20).reshape(5,4)\n",
    "B = torch.randn(12).reshape(4,3)\n",
    "A, B, torch.mm(A, B) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.tensor([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[  60,   70],\n",
       "           [ 160,  195]],\n",
       " \n",
       "          [[ 860,  920],\n",
       "           [1210, 1295]]],\n",
       " \n",
       " \n",
       "         [[[ 460,  570],\n",
       "           [ 560,  695]],\n",
       " \n",
       "          [[2260, 2420],\n",
       "           [2610, 2795]]]]),\n",
       " [[tensor([[ 60,  70],\n",
       "           [160, 195]]),\n",
       "   tensor([[ 860,  920],\n",
       "           [1210, 1295]])],\n",
       "  [tensor([[460, 570],\n",
       "           [560, 695]]),\n",
       "   tensor([[2260, 2420],\n",
       "           [2610, 2795]])]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"knowhow --- torch.matmul对高维Tensor输入的处理\n",
    "\n",
    "- 如果两个张量都是一维的,则返回它们的点积（标量）\n",
    "- 如果两个张量都是二维的,则返回它们的矩阵-矩阵乘积\n",
    "- 如果第一个参数是一维的,第二个参数是二维的,则在第一个参数的- 维度前面添加一个维度1,以便进行矩阵乘法。在矩阵相乘之后,前置维度会被移除\n",
    "- 如果第一个参数是二维的,第二个参数是一维的,则返回矩阵向量积\n",
    "- 如果两个参数至少有一个是N维(其中N > 2),则返回批处理矩阵乘法。在这种情况下,非矩阵维度会被广播\n",
    "\"\"\"\n",
    "\n",
    "a = torch.Tensor(torch.arange(0,40).reshape((2, 2, 2, 5)))\n",
    "b = torch.Tensor(torch.arange(0,20).reshape(2, 5, 2))\n",
    "a, b, a.matmul(b)\n",
    "a.matmul(b), [[torch.matmul(a[0][0], b[0]), torch.matmul(a[0][1], b[1])],\n",
    "              [torch.matmul(a[1][0], b[0]), torch.matmul(a[1][1], b[1])]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 证明一个矩阵$A$的转置的转置是$A$, 即$(A^T)^T = A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 给出两个矩阵$A$和$B$，证明“它们转置的和”等于“它们和的转置”，即$A^T+B^T = (A+B)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 给定任意方阵$A$，$A+A^T$总是对称的吗?为什么?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 本节中定义了形状`(2,3,4)`的张量X。`len(X)`的输出结果是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3040,  0.0740, -2.7376, -0.8703],\n",
       "          [-0.4783,  0.6012,  0.6722, -0.8620],\n",
       "          [ 1.4926, -1.5211,  0.7222,  0.6040]],\n",
       " \n",
       "         [[-0.7243,  0.1576, -0.2154, -0.7686],\n",
       "          [-0.5517,  1.0391,  0.5049, -0.3055],\n",
       "          [-0.1807, -0.5764, -0.1027, -0.1325]]]),\n",
       " 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(24).reshape(2,3,4)\n",
    "X, len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 对于任意形状的张量$X$,`len(X)`是否总是对应于$X$特定轴的长度?这个轴是什么?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 运行`A/A.sum(axis=1)`，看看会发生什么。请分析一下原因？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jack/Desktop/论文阅读/hands-on-AI/实践/2-预备知识/2.3-线性代数.ipynb 单元格 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jack/Desktop/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/hands-on-AI/%E5%AE%9E%E8%B7%B5/2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/2.3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m20\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m5\u001b[39m,\u001b[39m4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jack/Desktop/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/hands-on-AI/%E5%AE%9E%E8%B7%B5/2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/2.3-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m A\u001b[39m/\u001b[39mA\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "A = torch.randn(20).reshape(5,4)\n",
    "A/A.sum(axis=1) ## 需要keepdim=True,广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 考虑一个具有形状`(2,3,4)`的张量，在轴0、1、2上的求和输出是什么形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(24).reshape(2,3,4)\n",
    "A.sum(axis=0), A.sum(axis=1), A.sum(axis=2), \\\n",
    "    A.sum(axis=0).shape, A.sum(axis=1).shape, A.sum(axis=2).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
